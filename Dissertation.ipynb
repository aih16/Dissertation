{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1go5t8UeT4bVrdWXWOA7MpMxxYAnsMyQc",
      "authorship_tag": "ABX9TyMfVWcmET8wJYuRgsItAM46",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aih16/Dissertation/blob/master/Dissertation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHMOB0qfFWjC"
      },
      "source": [
        "Link to datasets\n",
        "https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/\n",
        "\n",
        "https://sanjayasubedi.com.np/nlp/nlp-feature-extraction/\n",
        "https://medium.com/@eiki1212/feature-extraction-in-natural-language-processing-with-python-59c7cdcaf064\n",
        "https://scikit-learn.org/stable/modules/feature_extraction.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06ANx3P0EakX",
        "outputId": "9b294b59-d3d8-4f6d-d634-53b9d2926285"
      },
      "source": [
        "# Imports relevent libraries\n",
        "# import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, nltk, string\n",
        "import csv\n",
        "import os\n",
        "\n",
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV8ggIUbH9Aq"
      },
      "source": [
        "Below code takes the training data and extracts text and tags separately, preprocesses the data and writes them to their own CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QpcgsIbKgoy"
      },
      "source": [
        "#@title Preprocessing\n",
        "# Sets path to directory\n",
        "# Change directory accordingly if downloading data yourself\n",
        "directory = r'drive/MyDrive/University/Masters/Dissertation/data/trainingData'\n",
        "directory2 = r'drive/MyDrive/University/Masters/Dissertation/data/testData'\n",
        "# Init the Wordnet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Creates blank training csv file\n",
        "with open(\"trainingData.csv\", \"w\") as my_empty_csv:\n",
        "  pass\n",
        "\n",
        "# Iterates through directory\n",
        "for filename in os.listdir(directory):\n",
        "  if filename.endswith(\".xml\"):\n",
        "    dataFileName = (os.path.join(directory, filename))\n",
        "\n",
        "    # Opens current file during iteration\n",
        "    with open(dataFileName,\"r\") as f:\n",
        "      # Reads the current document\n",
        "      data = f.read()\n",
        "\n",
        "    Bs_data = BeautifulSoup(data, \"xml\")\n",
        "\n",
        "    # Gets all text and tags from xml file and converts to string\n",
        "    b_text = Bs_data.find_all(\"TEXT\")\n",
        "    textString = str(b_text)\n",
        "\n",
        "    # Remove all the special characters\n",
        "    textString = re.sub(r'\\W', ' ', textString)\n",
        "\n",
        "    # Remove numbers from text\n",
        "    textString = re.sub('[0-9]', ' ', textString)\n",
        "\n",
        "    # remove all single characters\n",
        "    textString = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', textString)\n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    textString = re.sub(r'\\s+', ' ', textString, flags=re.I)\n",
        "\n",
        "    # Removes a specific string that was appearing in the trainingData\n",
        "    textString = re.sub('[___________________________________]', ' ', textString)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    textString = textString.lower()\n",
        "\n",
        "    # Tokeniser that divides a string into substrings by splitting on the specified string\n",
        "    tokenized_text = word_tokenize(textString)\n",
        "\n",
        "    # Lemmantizes the words in the tokenized and lemmantized text list\n",
        "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in tokenized_text])\n",
        "\n",
        "    # Retokenizes the word after lemmantization turns it into a string\n",
        "    tokenized_text = word_tokenize(lemmatized_output)\n",
        "\n",
        "    # Removes stopwords from the tokenized text\n",
        "    for word in tokenized_text:\n",
        "        if word in stopwords.words('english'):\n",
        "            tokenized_text.remove(word)\n",
        "\n",
        "    with open('trainingData.csv', 'a') as f:\n",
        "        # create the csv writer\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        # write a row to the csv file\n",
        "        writer.writerow(tokenized_text)\n",
        "\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "# Does the same as above but grabs the info from the test data folder instead of the training data folder\n",
        "# and appends to the end of the trainingData.csv\n",
        "for filename in os.listdir(directory2):\n",
        "  if filename.endswith(\".xml\"):\n",
        "    dataFileName = (os.path.join(directory2, filename))\n",
        "\n",
        "    # Opens current file during iteration\n",
        "    with open(dataFileName,\"r\") as f:\n",
        "      # Reads the current document\n",
        "      data = f.read()\n",
        "\n",
        "    Bs_data = BeautifulSoup(data, \"xml\")\n",
        "\n",
        "    # Gets all text and tags from xml file and converts to string\n",
        "    b_text = Bs_data.find_all(\"TEXT\")\n",
        "    textString = str(b_text)\n",
        "\n",
        "    # Remove all the special characters\n",
        "    textString = re.sub(r'\\W', ' ', textString)\n",
        "\n",
        "    # Remove numbers from text\n",
        "    textString = re.sub('[0-9]', ' ', textString)\n",
        "\n",
        "    # remove all single characters\n",
        "    textString = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', textString)\n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    textString = re.sub(r'\\s+', ' ', textString, flags=re.I)\n",
        "\n",
        "    # Removes a specific string that was appearing in the trainingData\n",
        "    textString = re.sub('[___________________________________]', ' ', textString)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    textString = textString.lower()\n",
        "\n",
        "    # Tokeniser that divides a string into substrings by splitting on the specified string\n",
        "    tokenized_text = word_tokenize(textString)\n",
        "\n",
        "    # Lemmantizes the words in the tokenized text list\n",
        "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in tokenized_text])\n",
        "\n",
        "    # Retokenizes the word after lemmantization turns it into a string\n",
        "    tokenized_text = word_tokenize(lemmatized_output)\n",
        "\n",
        "    # Removes stopwords from the tokenized and lemmantized text\n",
        "    for word in tokenized_text:\n",
        "        if word in stopwords.words('english'):\n",
        "            tokenized_text.remove(word)\n",
        "\n",
        "    with open('trainingData.csv', 'a') as f:\n",
        "        # create the csv writer\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        # write a row to the csv file\n",
        "        writer.writerow(tokenized_text)\n",
        "\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "abList = []\n",
        "adcadList = []\n",
        "alcoholList = []\n",
        "aspList = []\n",
        "creatinineList = []\n",
        "dietsuppList = []\n",
        "drugabuseList = []\n",
        "englishList = []\n",
        "hbaList = []\n",
        "ketoList = []\n",
        "majorList = []\n",
        "decisionList = []\n",
        "mi6List = []\n",
        "\n",
        "# Sets path to directory\n",
        "directory = r'drive/MyDrive/University/Masters/Dissertation/data/trainingData'\n",
        "\n",
        "# Creates blank training csv file\n",
        "with open(\"trainingData.csv\", \"r\") as my_empty_csv:\n",
        "  pass\n",
        "\n",
        "# Iterates through directory\n",
        "for filename in os.listdir(directory):\n",
        "  if filename.endswith(\".xml\"):\n",
        "    dataFileName = (os.path.join(directory, filename))\n",
        "\n",
        "    # Opens current file during iteration\n",
        "    with open(dataFileName,\"r\") as f:\n",
        "      data = f.read()\n",
        "\n",
        "    Bs_data = BeautifulSoup(data, \"xml\")\n",
        "\n",
        "    # Gets all tags from xml file and converts to string\n",
        "    b_tags = Bs_data.find_all(\"TAGS\")\n",
        "    tagString = str(b_tags)\n",
        "\n",
        "    # Removes certain characters and the first and last lines containing the words TAGS\n",
        "    tagString = re.sub('[\\<>/\"\\]\\[]', '',tagString)\n",
        "    tagString = re.sub(\"TAGS\", '', tagString)\n",
        "\n",
        "    # Grabs first line from each string, in this case the line being the tags refering to abdominal issues\n",
        "    abString = tagString.splitlines()[1]\n",
        "    # Removes all text that is not the tag\n",
        "    abString = abString.replace('ABDOMINAL met=', '')\n",
        "    # Appends tags to list for use as labels later\n",
        "    abList.append(abString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    adcadString = tagString.splitlines()[2]\n",
        "    adcadString = adcadString.replace('ADVANCED-CAD met=', '')\n",
        "    adcadList.append(adcadString.partition('\\n')[0])\n",
        "    \n",
        "\n",
        "    alcoholString = tagString.splitlines()[3]\n",
        "    alcoholString = alcoholString.replace('ALCOHOL-ABUSE met=', '')\n",
        "    alcoholList.append(alcoholString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    aspString = tagString.splitlines()[4]\n",
        "    aspString = aspString.replace('ASP-FOR-MI met=', '')\n",
        "    aspList.append(aspString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    creatinineString = tagString.splitlines()[5]\n",
        "    creatinineString = creatinineString.replace('CREATININE met=', '')\n",
        "    creatinineList.append(creatinineString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    dietsuppString = tagString.splitlines()[6]\n",
        "    dietsuppString = dietsuppString.replace('DIETSUPP-2MOS met=', '')\n",
        "    dietsuppList.append(dietsuppString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    drugabuseString = tagString.splitlines()[7]\n",
        "    drugabuseString = drugabuseString.replace('DRUG-ABUSE met=', '')\n",
        "    drugabuseList.append(drugabuseString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    englishString = tagString.splitlines()[8]\n",
        "    englishString = englishString.replace('ENGLISH met=', '')\n",
        "    englishList.append(englishString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    hbaString = tagString.splitlines()[9]\n",
        "    hbaString = hbaString.replace('HBA1C met=', '')\n",
        "    hbaList.append(hbaString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    ketoString = tagString.splitlines()[10]\n",
        "    ketoString = ketoString.replace('KETO-1YR met=', '')\n",
        "    ketoList.append(ketoString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    majorString = tagString.splitlines()[11]\n",
        "    majorString = majorString.replace('MAJOR-DIABETES met=', '')\n",
        "    majorList.append(majorString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    decisionString = tagString.splitlines()[12]\n",
        "    decisionString = decisionString.replace('MAKES-DECISIONS met=', '')\n",
        "    decisionList.append(decisionString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    mi6String = tagString.splitlines()[13]\n",
        "    mi6String = mi6String.replace('MI-6MOS met=', '')\n",
        "    mi6List.append(mi6String.partition('\\n')[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Iterates through directory of test data\n",
        "for filename in os.listdir(directory2):\n",
        "  if filename.endswith(\".xml\"):\n",
        "    dataFileName = (os.path.join(directory2, filename))\n",
        "\n",
        "    # Opens current file during iteration\n",
        "    with open(dataFileName,\"r\") as f:\n",
        "      data = f.read()\n",
        "\n",
        "    Bs_data = BeautifulSoup(data, \"xml\")\n",
        "\n",
        "    # Gets all tags from xml file and converts to string\n",
        "    b_tags = Bs_data.find_all(\"TAGS\")\n",
        "    tagString = str(b_tags)\n",
        "\n",
        "    # Removes certain characters and the first and last lines containing the words TAGS\n",
        "    tagString = re.sub('[\\<>/\"\\]\\[]', '',tagString)\n",
        "    tagString = re.sub(\"TAGS\", '', tagString)\n",
        "\n",
        "    # Grabs first line from each string, in this case the line being the tags refering to abdominal issues\n",
        "    abString = tagString.splitlines()[1]\n",
        "    # Removes all text that is not the tag\n",
        "    abString = abString.replace('ABDOMINAL met=', '')\n",
        "    # Appends tags to list for use as labels later\n",
        "    abList.append(abString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    adcadString = tagString.splitlines()[2]\n",
        "    adcadString = adcadString.replace('ADVANCED-CAD met=', '')\n",
        "    adcadList.append(adcadString.partition('\\n')[0])\n",
        "    \n",
        "\n",
        "    alcoholString = tagString.splitlines()[3]\n",
        "    alcoholString = alcoholString.replace('ALCOHOL-ABUSE met=', '')\n",
        "    alcoholList.append(alcoholString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    aspString = tagString.splitlines()[4]\n",
        "    aspString = aspString.replace('ASP-FOR-MI met=', '')\n",
        "    aspList.append(aspString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    creatinineString = tagString.splitlines()[5]\n",
        "    creatinineString = creatinineString.replace('CREATININE met=', '')\n",
        "    creatinineList.append(creatinineString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    dietsuppString = tagString.splitlines()[6]\n",
        "    dietsuppString = dietsuppString.replace('DIETSUPP-2MOS met=', '')\n",
        "    dietsuppList.append(dietsuppString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    drugabuseString = tagString.splitlines()[7]\n",
        "    drugabuseString = drugabuseString.replace('DRUG-ABUSE met=', '')\n",
        "    drugabuseList.append(drugabuseString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    englishString = tagString.splitlines()[8]\n",
        "    englishString = englishString.replace('ENGLISH met=', '')\n",
        "    englishList.append(englishString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    hbaString = tagString.splitlines()[9]\n",
        "    hbaString = hbaString.replace('HBA1C met=', '')\n",
        "    hbaList.append(hbaString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    ketoString = tagString.splitlines()[10]\n",
        "    ketoString = ketoString.replace('KETO-1YR met=', '')\n",
        "    ketoList.append(ketoString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    majorString = tagString.splitlines()[11]\n",
        "    majorString = majorString.replace('MAJOR-DIABETES met=', '')\n",
        "    majorList.append(majorString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    decisionString = tagString.splitlines()[12]\n",
        "    decisionString = decisionString.replace('MAKES-DECISIONS met=', '')\n",
        "    decisionList.append(decisionString.partition('\\n')[0])\n",
        "\n",
        "\n",
        "    mi6String = tagString.splitlines()[13]\n",
        "    mi6String = mi6String.replace('MI-6MOS met=', '')\n",
        "    mi6List.append(mi6String.partition('\\n')[0])\n",
        "\n",
        "variableList = [abList,adcadList,alcoholList,aspList,creatinineList,dietsuppList,drugabuseList,englishList,hbaList,ketoList,majorList,decisionList,mi6List]\n",
        "listNames = [\"abList\",\"adcadList\",\"alcoholList\",\"aspList\",\"creatinineList\",\"dietsuppList\",\"drugabuseList\",\"englishList\",\"hbaList\",\"ketoList\",\"majorList\",\"decisionList\",\"mi6List\"]\n",
        "\n",
        "\n",
        "for i, j in zip(variableList, listNames):\n",
        "  df = pd.DataFrame(i)\n",
        "  df.to_csv(\"%s.csv\" % j, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXgfI7gJM1Jc"
      },
      "source": [
        "# Quick method to look at the data within the trainingData.csv\n",
        "# to make sure the above block of code correctly\n",
        "# removed unneeded characters, numbers, etc\n",
        "\n",
        "with open(\"trainingData.csv\",\"r\") as myfile:\n",
        "  # Reads the current document\n",
        "  head = [next(myfile) for x in range(10)]\n",
        "  print(head)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRyOii-WItQN"
      },
      "source": [
        "Below are 4 blocks of code, each one running a separate machine learning system, these being:\n",
        "- GuassianNB\n",
        "- MultinomialNB\n",
        "- Logistic Regression\n",
        "- SVM\n",
        "\n",
        "Each one has both a countVectorizer and a Tfidf vectorizer for extracting features into the countFeatures.csv and tfidfFeatures.csv."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OJ7s3X_vLXX"
      },
      "source": [
        "#@title Gaussian NB\n",
        "# Uses CountVectorizer for extracting features\n",
        "with open(\"trainingData.csv\", 'r', encoding='utf-8') as vectorData:\n",
        "\n",
        "    userInput = int(input(\"Select max amount of features to extract: \"))\n",
        "\n",
        "    # Create the transformer, 0.7 max df will only use features that appear in less than 70% of the documents\n",
        "    # Words that occur in almost every document are usually not suitable for classification\n",
        "    # Such as record, text and date appearing at the start of every document\n",
        "    vectorizer = CountVectorizer(max_features=userInput, max_df=0.7)\n",
        "\n",
        "    corpus = vectorData\n",
        "\n",
        "    # tokenize and builds vocab\n",
        "    vector = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # summarize encoded vector\n",
        "    print(vector.shape)\n",
        "    print(vectorizer.get_feature_names())\n",
        "\n",
        "    # creates array from extracted features\n",
        "    vectorArray = vector.toarray()\n",
        "    print(vectorArray)\n",
        "\n",
        "    # saves the array of features\n",
        "    data = asarray(vectorArray)\n",
        "    savetxt('countFeatures.csv', data, delimiter=',')\n",
        "\n",
        "# Uses Tfidf for extracting features\n",
        "with open(\"trainingData.csv\", 'r', encoding='utf-8') as vectorData:\n",
        "\n",
        "  # documents = vectorData.read()\n",
        "  tfidfconverter = TfidfVectorizer(max_features=userInput, max_df=0.7)\n",
        "  tfidf = tfidfconverter.fit_transform(vectorData).toarray()\n",
        "  savetxt('tfidfFeatures.csv', tfidf, delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Finds the longest row/row with the biggest numbers of columns, creates column names\n",
        "selectionMethod = (input(\"Select CSV feature file to use: \"))\n",
        "\n",
        "# Input\n",
        "data_file = selectionMethod\n",
        "\n",
        "# Delimiter\n",
        "data_file_delimiter = ','\n",
        "\n",
        "# The max column count a line in the file could have\n",
        "largest_column_count = 0\n",
        "\n",
        "# Loop the data lines\n",
        "with open(data_file, 'r') as temp_f:\n",
        "    # Read the lines\n",
        "    lines = temp_f.readlines()\n",
        "\n",
        "    for l in lines:\n",
        "        # Count the column count for the current line\n",
        "        column_count = len(l.split(data_file_delimiter)) + 1\n",
        "        \n",
        "        # Set the new most column count\n",
        "        largest_column_count = column_count if largest_column_count < column_count else largest_column_count\n",
        "\n",
        "# Generate column names (will be 0, 1, 2, ..., largest_column_count - 1)\n",
        "column_names = [i for i in range(0, largest_column_count)]\n",
        "\n",
        "# Read in CSV as DataFrame and fills in NaN with 0\n",
        "trainDF = pd.read_csv(data_file, header=None, delimiter=data_file_delimiter, names=column_names)\n",
        "trainDF.fillna(0, inplace=True), trainDF.drop(trainDF.columns[len(trainDF.columns)-1], axis=1, inplace=True)\n",
        "\n",
        "# Amends the target (met/not met list) to the end of the file\n",
        "targetDF = pd.read_csv(input(\"Pick a CSV file to amend: \"))\n",
        "trainDF['Target'] = targetDF\n",
        "\n",
        "# Converts the target \"met\" and \"not met\" to 1s and 0s\n",
        "trainDF['Target'] = trainDF['Target'].replace(['not met','met'],['0','1'])\n",
        "\n",
        "# Sets the data as the X value and the target as the Y value\n",
        "Y = trainDF.iloc[:,-1:]\n",
        "X = trainDF.loc[:, trainDF.columns != 'Target']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Asks user for the number of tests to run\n",
        "runTimes = int(input(\"Number of test runs: \"))\n",
        "\n",
        "print(\"Running Gaussian Naive Bayes\")\n",
        "totalAccuracy = 0\n",
        "for x in range(runTimes):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25,shuffle=True)\n",
        "\n",
        "  # trains the GaussianNB model\n",
        "  model = GaussianNB()\n",
        "  model.fit(x_train, y_train)\n",
        "\n",
        "  # predicts the y outcome using the test data\n",
        "  y_pred = model.predict(x_test)\n",
        "  # print(y_pred)\n",
        "\n",
        "  # gives accuracy of prediction\n",
        "  accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "  # print(accuracy)\n",
        "  totalAccuracy = totalAccuracy + accuracy\n",
        "\n",
        "# provides mean accuracy after x amount of runs\n",
        "meanAccuracy = totalAccuracy/runTimes\n",
        "print(\"Average Accuracy after\", runTimes, \"tests: \", meanAccuracy)\n",
        "\n",
        "print(\"Confusion matrix \\n\", confusion_matrix(y_test,y_pred))\n",
        "print(\"Classification report \\n\", classification_report(y_test,y_pred))\n",
        "print(\"Accuracy count \\n\", accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7g0Va6zvQeX"
      },
      "source": [
        "#@title Multinomial NB\n",
        "# Uses CountVectorizer for extracting features\n",
        "with open(\"trainingData.csv\", 'r', encoding='utf-8') as vectorData:\n",
        "\n",
        "    userInput = int(input(\"Select max amount of features to extract: \"))\n",
        "\n",
        "    # Create the transformer, 0.7 max df will only use features that appear in less than 70% of the documents\n",
        "    # Words that occur in almost every document are usually not suitable for classification\n",
        "    # Such as record, text and date appearing at the start of every document\n",
        "    vectorizer = CountVectorizer(max_features=userInput, max_df=0.7)\n",
        "\n",
        "    corpus = vectorData\n",
        "\n",
        "    # tokenize and builds vocab\n",
        "    vector = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # summarize encoded vector\n",
        "    print(vector.shape)\n",
        "    print(vectorizer.get_feature_names())\n",
        "\n",
        "    # creates array from extracted features\n",
        "    vectorArray = vector.toarray()\n",
        "    print(vectorArray)\n",
        "\n",
        "    # saves the array of features\n",
        "    data = asarray(vectorArray)\n",
        "    savetxt('countFeatures.csv', data, delimiter=',')\n",
        "\n",
        "# Uses Tfidf for extracting features\n",
        "with open(\"trainingData.csv\", 'r', encoding='utf-8') as vectorData:\n",
        "\n",
        "  # documents = vectorData.read()\n",
        "  tfidfconverter = TfidfVectorizer(max_features=100, max_df=0.7)\n",
        "  X = tfidfconverter.fit_transform(vectorData).toarray()\n",
        "  savetxt('tfidfFeatures.csv', data, delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Finds the longest row/row with the biggest numbers of columns, creates column names\n",
        "selectionMethod = (input(\"Select CSV feature file to use: \"))\n",
        "\n",
        "# Input\n",
        "data_file = selectionMethod\n",
        "\n",
        "# Delimiter\n",
        "data_file_delimiter = ','\n",
        "\n",
        "# The max column count a line in the file could have\n",
        "largest_column_count = 0\n",
        "\n",
        "# Loop the data lines\n",
        "with open(data_file, 'r') as temp_f:\n",
        "    # Read the lines\n",
        "    lines = temp_f.readlines()\n",
        "\n",
        "    for l in lines:\n",
        "        # Count the column count for the current line\n",
        "        column_count = len(l.split(data_file_delimiter)) + 1\n",
        "        \n",
        "        # Set the new most column count\n",
        "        largest_column_count = column_count if largest_column_count < column_count else largest_column_count\n",
        "\n",
        "# Generate column names (will be 0, 1, 2, ..., largest_column_count - 1)\n",
        "column_names = [i for i in range(0, largest_column_count)]\n",
        "\n",
        "# Read in CSV as DataFrame and fills in NaN with 0\n",
        "trainDF = pd.read_csv(data_file, header=None, delimiter=data_file_delimiter, names=column_names)\n",
        "trainDF.fillna(0), trainDF.drop(trainDF.columns[len(trainDF.columns)-1], axis=1, inplace=True)\n",
        "\n",
        "# Amends the target (met/not met list) to the end of the file\n",
        "targetDF = pd.read_csv(input(\"Pick a CSV file to amend: \"))\n",
        "trainDF['Target'] = targetDF\n",
        "\n",
        "# Converts the target \"met\" and \"not met\" to 1s and 0s\n",
        "trainDF['Target'] = trainDF['Target'].replace(['not met','met'],['0','1'])\n",
        "\n",
        "# Sets the data as the X value and the target as the Y value\n",
        "Y = trainDF.iloc[:,-1:]\n",
        "X = trainDF.loc[:, trainDF.columns != 'Target']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Asks user for the number of tests to run\n",
        "runTimes = int(input(\"Number of test runs: \"))\n",
        "\n",
        "print(\"Running Multinomial Naive Bayes\")\n",
        "totalAccuracy = 0\n",
        "for x in range(runTimes):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25,shuffle=True)\n",
        "\n",
        "  # trains the GaussianNB model\n",
        "  model = MultinomialNB()\n",
        "  model.fit(x_train, y_train)\n",
        "\n",
        "  # predicts the y outcome using the test data\n",
        "  y_pred = model.predict(x_test)\n",
        "  # print(y_pred)\n",
        "\n",
        "  # gives accuracy of prediction\n",
        "  accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "  # print(accuracy)\n",
        "  totalAccuracy = totalAccuracy + accuracy\n",
        "\n",
        "# provides mean accuracy after x amount of runs\n",
        "meanAccuracy = totalAccuracy/runTimes\n",
        "print(\"Average Accuracy after\", runTimes, \"tests: \", meanAccuracy)\n",
        "\n",
        "print(\"Confusion matrix \\n\", confusion_matrix(y_test,y_pred))\n",
        "print(\"Classification report \\n\", classification_report(y_test,y_pred))\n",
        "print(\"Accuracy count \\n\", accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHxoTvVXY13g"
      },
      "source": [
        "#@title Logistic Regression\n",
        "# Uses CountVectorizer for extracting features\n",
        "with open(\"trainingData.csv\", 'r', encoding='utf-8') as vectorData:\n",
        "\n",
        "    userInput = int(input(\"Select max amount of features to extract: \"))\n",
        "\n",
        "    # create the transform\n",
        "    vectorizer = CountVectorizer(max_features=userInput, max_df=0.7)\n",
        "\n",
        "    corpus = vectorData\n",
        "\n",
        "    # tokenize and builds vocab\n",
        "    vector = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # summarize encoded vector\n",
        "    print(vector.shape)\n",
        "    print(vectorizer.get_feature_names())\n",
        "\n",
        "    # creates array from extracted features\n",
        "    vectorArray = vector.toarray()\n",
        "    print(vectorArray)\n",
        "\n",
        "    # saves the array of features\n",
        "    data = asarray(vectorArray)\n",
        "    savetxt('countFeatures.csv', data, delimiter=',')\n",
        "\n",
        "# Uses Tfidf for extracting features\n",
        "with open(\"trainingData.csv\", 'r', encoding='utf-8') as vectorData:\n",
        "\n",
        "  # documents = vectorData.read()\n",
        "  tfidfconverter = TfidfVectorizer(max_features=100, max_df=0.7)\n",
        "  X = tfidfconverter.fit_transform(vectorData).toarray()\n",
        "  savetxt('tfidfFeatures.csv', data, delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Finds the longest row/row with the biggest numbers of columns, creates column names\n",
        "selectionMethod = (input(\"Select CSV feature file to use: \"))\n",
        "\n",
        "# Input\n",
        "data_file = selectionMethod\n",
        "\n",
        "# Delimiter\n",
        "data_file_delimiter = ','\n",
        "\n",
        "# The max column count a line in the file could have\n",
        "largest_column_count = 0\n",
        "\n",
        "# Loop the data lines\n",
        "with open(data_file, 'r') as temp_f:\n",
        "    # Read the lines\n",
        "    lines = temp_f.readlines()\n",
        "\n",
        "    for l in lines:\n",
        "        # Count the column count for the current line\n",
        "        column_count = len(l.split(data_file_delimiter)) + 1\n",
        "        \n",
        "        # Set the new most column count\n",
        "        largest_column_count = column_count if largest_column_count < column_count else largest_column_count\n",
        "\n",
        "# Generate column names (will be 0, 1, 2, ..., largest_column_count - 1)\n",
        "column_names = [i for i in range(0, largest_column_count)]\n",
        "\n",
        "# Read in CSV as DataFrame\n",
        "trainDF = pd.read_csv(data_file, header=None, delimiter=data_file_delimiter, names=column_names)\n",
        "trainDF.fillna(0), trainDF.drop(trainDF.columns[len(trainDF.columns)-1], axis=1, inplace=True)\n",
        "\n",
        "# Amends the target (met/not met list) to the end of the file\n",
        "targetDF = pd.read_csv(input(\"Pick a CSV file to amend: \"))\n",
        "trainDF['Target'] = targetDF\n",
        "\n",
        "trainDF['Target'] = trainDF['Target'].replace(['not met','met'],['0','1'])\n",
        "\n",
        "# Sets the data as the X value and the target as the Y value\n",
        "Y = trainDF.iloc[:,-1:]\n",
        "X = trainDF.loc[:, trainDF.columns != 'Target']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Asks user for the number of tests to run\n",
        "runTimes = int(input(\"Number of test runs: \"))\n",
        "\n",
        "print(\"Running Logistic Regression\")\n",
        "totalAccuracy = 0\n",
        "for x in range(runTimes):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25,shuffle=True)\n",
        "\n",
        "  # trains the GaussianNB model\n",
        "  model = LogisticRegression(max_iter=250)\n",
        "  model.fit(x_train, y_train.values.ravel())\n",
        "\n",
        "  # predicts the y outcome using the test data\n",
        "  y_pred = model.predict(x_test)\n",
        "  # print(y_pred)\n",
        "\n",
        "  # gives accuracy of prediction\n",
        "  accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "  # print(accuracy)\n",
        "  totalAccuracy = totalAccuracy + accuracy\n",
        "\n",
        "# provides mean accuracy after x amount of runs\n",
        "meanAccuracy = totalAccuracy/runTimes\n",
        "print(\"Average Accuracy after\", runTimes, \"tests: \", meanAccuracy)\n",
        "\n",
        "print(\"Confusion matrix \\n\", confusion_matrix(y_test,y_pred))\n",
        "print(\"Classification report \\n\", classification_report(y_test,y_pred))\n",
        "print(\"Accuracy count \\n\", accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLAnWbFMksdG"
      },
      "source": [
        "#@title SVM\n",
        "# Uses CountVectorizer for extracting features\n",
        "with open(\"trainingData.csv\", 'r', encoding='utf-8') as vectorData:\n",
        "\n",
        "    userInput = int(input(\"Select max amount of features to extract: \"))\n",
        "\n",
        "    # create the transform\n",
        "    vectorizer = CountVectorizer(max_features=userInput, max_df=0.7)\n",
        "\n",
        "    corpus = vectorData\n",
        "\n",
        "    # tokenize and builds vocab\n",
        "    vector = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # summarize encoded vector\n",
        "    print(vector.shape)\n",
        "    print(vectorizer.get_feature_names())\n",
        "\n",
        "    # creates array from extracted features\n",
        "    vectorArray = vector.toarray()\n",
        "    print(vectorArray)\n",
        "\n",
        "    # saves the array of features\n",
        "    data = asarray(vectorArray)\n",
        "    savetxt('countFeatures.csv', data, delimiter=',')\n",
        "\n",
        "# Uses Tfidf for extracting features\n",
        "with open(\"trainingData.csv\", 'r', encoding='utf-8') as vectorData:\n",
        "\n",
        "  # documents = vectorData.read()\n",
        "  tfidfconverter = TfidfVectorizer(max_features=100, max_df=0.7)\n",
        "  X = tfidfconverter.fit_transform(vectorData).toarray()\n",
        "  savetxt('tfidfFeatures.csv', data, delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Finds the longest row/row with the biggest numbers of columns, creates column names\n",
        "selectionMethod = (input(\"Select CSV feature file to use: \"))\n",
        "\n",
        "# Input\n",
        "data_file = selectionMethod\n",
        "\n",
        "# Delimiter\n",
        "data_file_delimiter = ','\n",
        "\n",
        "# The max column count a line in the file could have\n",
        "largest_column_count = 0\n",
        "\n",
        "# Loop the data lines\n",
        "with open(data_file, 'r') as temp_f:\n",
        "    # Read the lines\n",
        "    lines = temp_f.readlines()\n",
        "\n",
        "    for l in lines:\n",
        "        # Count the column count for the current line\n",
        "        column_count = len(l.split(data_file_delimiter)) + 1\n",
        "        \n",
        "        # Set the new most column count\n",
        "        largest_column_count = column_count if largest_column_count < column_count else largest_column_count\n",
        "\n",
        "# Generate column names (will be 0, 1, 2, ..., largest_column_count - 1)\n",
        "column_names = [i for i in range(0, largest_column_count)]\n",
        "\n",
        "# Read in CSV as DataFrame\n",
        "trainDF = pd.read_csv(data_file, header=None, delimiter=data_file_delimiter, names=column_names)\n",
        "trainDF.fillna(0), trainDF.drop(trainDF.columns[len(trainDF.columns)-1], axis=1, inplace=True)\n",
        "\n",
        "# Amends the target (met/not met list) to the end of the file\n",
        "targetDF = pd.read_csv(input(\"Pick a CSV file to amend: \"))\n",
        "trainDF['Target'] = targetDF\n",
        "\n",
        "trainDF['Target'] = trainDF['Target'].replace(['not met','met'],['0','1'])\n",
        "\n",
        "# Sets the data as the X value and the target as the Y value\n",
        "Y = trainDF.iloc[:,-1:]\n",
        "X = trainDF.loc[:, trainDF.columns != 'Target']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Asks user for the number of tests to run\n",
        "runTimes = int(input(\"Number of test runs: \"))\n",
        "\n",
        "print(\"Running SVM\")\n",
        "totalAccuracy = 0\n",
        "for x in range(runTimes):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25,shuffle=True)\n",
        "\n",
        "  # trains the GaussianNB model\n",
        "  clf = svm.SVC(kernel=\"linear\")\n",
        "  clf.fit(x_train, y_train)\n",
        "\n",
        "  # predicts the y outcome using the test data\n",
        "  y_pred = clf.predict(x_test)\n",
        "  # print(y_pred)\n",
        "\n",
        "  # gives accuracy of prediction\n",
        "  accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "  # print(accuracy)\n",
        "  totalAccuracy = totalAccuracy + accuracy\n",
        "\n",
        "# provides mean accuracy after x amount of runs\n",
        "meanAccuracy = totalAccuracy/runTimes\n",
        "print(\"Average Accuracy after\", runTimes, \"tests: \", meanAccuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phih31LfEYSX"
      },
      "source": [
        "#@title Run all ML models\n",
        "# Allows user to set different amount of features to extract from the training data\n",
        "featuresList = [1000, 5000, 10000]\n",
        "userChoice = (input(\"Pick a CSV file to amend: \"))\n",
        "targetDF = pd.read_csv(userChoice)\n",
        "\n",
        "runTimes = 1\n",
        "for i in featuresList:\n",
        "  resultsDirectory = r'drive/MyDrive/University/Masters/Dissertation/results'\n",
        "\n",
        "  # Uses CountVectorizer for extracting features\n",
        "  with open(\"trainingData.csv\", 'r', encoding='utf-8') as vectorData:\n",
        "\n",
        "      # userInput = int(input(\"Select max amount of features to extract: \"))\n",
        "      userInput = i\n",
        "\n",
        "      # Create the transformer, 0.7 max df will only use features that appear in less than 70% of the documents\n",
        "      # Words that occur in almost every document are usually not suitable for classification\n",
        "      # Such as record, text and date appearing at the start of every document\n",
        "      vectorizer = CountVectorizer(max_features=userInput, ngram_range=(1,2), max_df=0.7)\n",
        "\n",
        "      corpus = vectorData\n",
        "\n",
        "      # tokenize and builds vocab\n",
        "      vector = vectorizer.fit_transform(corpus)\n",
        "\n",
        "      # summarize encoded vector\n",
        "      print(\"Printing Count Vectorizer Info:\")\n",
        "      print(vector.shape)\n",
        "      print(vectorizer.get_feature_names())\n",
        "\n",
        "      # creates array from extracted features\n",
        "      vectorArray = vector.toarray()\n",
        "      print(vectorArray, \"\\n\")\n",
        "\n",
        "      # saves the array of features\n",
        "      data = asarray(vectorArray)\n",
        "      savetxt('countFeatures.csv', data, delimiter=',')\n",
        "\n",
        "  # Uses Tfidf for extracting features\n",
        "  with open(\"trainingData.csv\", 'r', encoding='utf-8') as vectorData:\n",
        "\n",
        "    # documents = vectorData.read()\n",
        "    tfidfconverter = TfidfVectorizer(max_features=userInput, ngram_range=(1,2), max_df=0.7, min_df=0.1)\n",
        "    tfidf = tfidfconverter.fit_transform(vectorData).toarray()\n",
        "\n",
        "    print(\"Printing tfidf Vectorizer Info:\")\n",
        "    print(tfidf.shape)\n",
        "    print(tfidfconverter.get_feature_names())\n",
        "\n",
        "    print(tfidf, \"\\n\")\n",
        "\n",
        "    savetxt('tfidfFeatures.csv', tfidf, delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Finds the longest row/row with the biggest numbers of columns, creates column names\n",
        "  # selectionMethod = (input(\"Select CSV feature file to use: \"))\n",
        "  selectionMethod = \"tfidfFeatures.csv\"\n",
        "\n",
        "  # Delimiter\n",
        "  data_file_delimiter = ','\n",
        "\n",
        "  # Read in CSV as DataFrame and fills in NaN with 0\n",
        "  trainDF = pd.read_csv(selectionMethod, header=None, delimiter=data_file_delimiter)\n",
        "  trainDF.fillna(0), trainDF.drop(trainDF.columns[len(trainDF.columns)-1], axis=1, inplace=True)\n",
        "\n",
        "  # Amends the target (met/not met list) to the end of the file\n",
        "  trainDF['Target'] = targetDF\n",
        "\n",
        "  # Converts the target \"met\" and \"not met\" to 1s and 0s\n",
        "  trainDF['Target'] = trainDF['Target'].replace(['not met','met'],['0','1'])\n",
        "\n",
        "  # Separates the training and testing data sets\n",
        "  testDF = trainDF.tail(86)\n",
        "  trainDF.drop(df.tail(86).index,inplace=True)\n",
        "\n",
        "  # Sets the data as the X value and the target as the Y value\n",
        "  y_train = trainDF.iloc[:,-1:]\n",
        "  x_train = trainDF.loc[:, trainDF.columns != 'Target']\n",
        "\n",
        "  y_test = testDF.iloc[:,-1:]\n",
        "  x_test = testDF.loc[:, testDF.columns != 'Target']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(\"Running Gaussian Naive Bayes\")\n",
        "  GNBtotalAccuracy = 0\n",
        "  for x in range(runTimes):\n",
        "\n",
        "    GNBmodel = GaussianNB()\n",
        "    GNBmodel.fit(x_train, y_train.values.ravel())\n",
        "    GNBy_pred = GNBmodel.predict(x_test)\n",
        "    GNBaccuracy = accuracy_score(y_test, GNBy_pred) * 100\n",
        "    GNBtotalAccuracy = GNBtotalAccuracy + GNBaccuracy\n",
        "\n",
        "  GNBmeanAccuracy = GNBtotalAccuracy/runTimes\n",
        "\n",
        "\n",
        "  print(\"Running Multinomial Naive Bayes\")\n",
        "  MNBtotalAccuracy = 0\n",
        "  for x in range(runTimes):\n",
        "\n",
        "    MNBmodel = MultinomialNB()\n",
        "    MNBmodel.fit(x_train, y_train.values.ravel())\n",
        "    MNBy_pred = MNBmodel.predict(x_test)\n",
        "    MNBaccuracy = accuracy_score(y_test, MNBy_pred) * 100\n",
        "    MNBtotalAccuracy = MNBtotalAccuracy + MNBaccuracy\n",
        "\n",
        "  MNBmeanAccuracy = MNBtotalAccuracy/runTimes\n",
        "\n",
        "\n",
        "\n",
        "  print(\"Running Logistic Regression\")\n",
        "  LRtotalAccuracy = 0\n",
        "  for x in range(runTimes):\n",
        "\n",
        "    LRmodel = LogisticRegression(max_iter=1000)\n",
        "    LRmodel.fit(x_train, y_train.values.ravel())\n",
        "    LRy_pred = LRmodel.predict(x_test)\n",
        "    LRaccuracy = accuracy_score(y_test, LRy_pred) * 100\n",
        "    LRtotalAccuracy = LRtotalAccuracy + LRaccuracy\n",
        "\n",
        "  LRmeanAccuracy = LRtotalAccuracy/runTimes\n",
        "\n",
        "\n",
        "\n",
        "  print(\"Running SVM\")\n",
        "  SVMtotalAccuracy = 0\n",
        "  for x in range(runTimes):\n",
        "\n",
        "    SVMmodel = svm.SVC(kernel=\"linear\")\n",
        "    SVMmodel.fit(x_train, y_train.values.ravel())\n",
        "    SVMy_pred = SVMmodel.predict(x_test)\n",
        "    SVMaccuracy = accuracy_score(y_test, SVMy_pred) * 100\n",
        "    SVMtotalAccuracy = SVMtotalAccuracy + SVMaccuracy\n",
        "\n",
        "  SVMmeanAccuracy = SVMtotalAccuracy/runTimes\n",
        "\n",
        "  # Removes .csv from file name when writing to new file\n",
        "  userChoice = os.path.splitext(userChoice)[0]\n",
        "\n",
        "  name_of_file = \"%s %s Features.txt\" % (userChoice, userInput)\n",
        "  save_path = r'drive/MyDrive/University/Masters/Dissertation/Results'\n",
        "  complete_name = os.path.join(save_path, name_of_file)\n",
        "\n",
        "  with open(complete_name,'w') as f:\n",
        "    f.write(str(userInput) + \" features and \" + str(runTimes) + \" test runs using the \" + str(userChoice) + \" file.\")\n",
        "    f.write(\"\\n GNB average Accuracy \" + str(GNBmeanAccuracy))\n",
        "    f.write(\"\\n MNB average Accuracy \" + str(MNBmeanAccuracy))\n",
        "    f.write(\"\\n LR average Accuracy \" + str(LRmeanAccuracy))\n",
        "    f.write(\"\\n SVM average Accuracy \" + str(SVMmeanAccuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIk7Jx47Jert"
      },
      "source": [
        "#@title Run all ML models simplified\n",
        "# Allows user to set different amount of features to extract from the training data\n",
        "\n",
        "userChoice = (input(\"Pick a CSV file to amend: \"))\n",
        "targetDF = pd.read_csv(userChoice)\n",
        "resultsDirectory = r'drive/MyDrive/University/Masters/Dissertation/results'\n",
        "\n",
        "# Uses Tfidf for extracting features\n",
        "with open(\"trainingData.csv\", 'r', encoding='utf-8') as vectorData:\n",
        "\n",
        "  # documents = vectorData.read()\n",
        "  tfidfconverter = TfidfVectorizer(ngram_range=(1,2), max_df=0.7, min_df=0.1)\n",
        "  tfidf = tfidfconverter.fit_transform(vectorData).toarray()\n",
        "\n",
        "  print(\"Printing tfidf Vectorizer Info:\")\n",
        "  print(tfidf.shape)\n",
        "  print(tfidfconverter.get_feature_names())\n",
        "\n",
        "  print(tfidf, \"\\n\")\n",
        "\n",
        "  savetxt('tfidfFeatures.csv', tfidf, delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Finds the longest row/row with the biggest numbers of columns, creates column names\n",
        "# selectionMethod = (input(\"Select CSV feature file to use: \"))\n",
        "selectionMethod = \"tfidfFeatures.csv\"\n",
        "\n",
        "# Delimiter\n",
        "data_file_delimiter = ','\n",
        "\n",
        "# Read in CSV as DataFrame and fills in NaN with 0\n",
        "trainDF = pd.read_csv(selectionMethod, header=None, delimiter=data_file_delimiter)\n",
        "trainDF.fillna(0), trainDF.drop(trainDF.columns[len(trainDF.columns)-1], axis=1, inplace=True)\n",
        "\n",
        "# Amends the target (met/not met list) to the end of the file\n",
        "trainDF['Target'] = targetDF\n",
        "\n",
        "# Converts the target \"met\" and \"not met\" to 1s and 0s\n",
        "trainDF['Target'] = trainDF['Target'].replace(['not met','met'],['0','1'])\n",
        "\n",
        "# Separates the training and testing data sets\n",
        "testDF = trainDF.tail(86)\n",
        "trainDF.drop(trainDF.tail(86).index,inplace=True)\n",
        "\n",
        "# Sets the data as the X value and the target as the Y value\n",
        "y_train = trainDF.iloc[:,-1:]\n",
        "x_train = trainDF.loc[:, trainDF.columns != 'Target']\n",
        "\n",
        "y_test = testDF.iloc[:,-1:]\n",
        "x_test = testDF.loc[:, testDF.columns != 'Target']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Running Gaussian Naive Bayes\")\n",
        "GNBmodel = GaussianNB()\n",
        "GNBmodel.fit(x_train, y_train.values.ravel())\n",
        "GNBy_pred = GNBmodel.predict(x_test)\n",
        "GNBaccuracy = accuracy_score(y_test, GNBy_pred) * 100\n",
        "GNBreport = classification_report(y_test,GNBy_pred)\n",
        "\n",
        "\n",
        "print(\"Running Multinomial Naive Bayes\")\n",
        "MNBmodel = MultinomialNB()\n",
        "MNBmodel.fit(x_train, y_train.values.ravel())\n",
        "MNBy_pred = MNBmodel.predict(x_test)\n",
        "MNBaccuracy = accuracy_score(y_test, MNBy_pred) * 100\n",
        "MNBreport = classification_report(y_test,MNBy_pred)\n",
        "\n",
        "print(\"Running Logistic Regression\")\n",
        "LRmodel = LogisticRegression(max_iter=1000)\n",
        "LRmodel.fit(x_train, y_train.values.ravel())\n",
        "LRy_pred = LRmodel.predict(x_test)\n",
        "LRaccuracy = accuracy_score(y_test, LRy_pred) * 100\n",
        "LRreport = classification_report(y_test,LRy_pred)\n",
        "\n",
        "print(\"Running SVM\")\n",
        "SVMmodel = svm.SVC(kernel=\"linear\")\n",
        "SVMmodel.fit(x_train, y_train.values.ravel())\n",
        "SVMy_pred = SVMmodel.predict(x_test)\n",
        "SVMaccuracy = accuracy_score(y_test, SVMy_pred) * 100\n",
        "SVMreport = classification_report(y_test,SVMy_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Removes .csv from file name when writing to new file\n",
        "userChoice = os.path.splitext(userChoice)[0]\n",
        "\n",
        "numFeatures = MNBmodel.coef_.shape[-1]\n",
        "name_of_file = \"%s %s Features.txt\" % (userChoice, numFeatures)\n",
        "save_path = r'drive/MyDrive/University/Masters/Dissertation/Results'\n",
        "complete_name = os.path.join(save_path, name_of_file)\n",
        "\n",
        "with open(complete_name,'w') as f:\n",
        "  f.write(str(numFeatures) + \" features \" + str(userChoice) + \" file.\")\n",
        "  f.write(\"\\n GNB average Accuracy \" + str(GNBaccuracy) + \"\\n\" + GNBreport)\n",
        "  f.write(\"\\n MNB average Accuracy \" + str(MNBaccuracy) + \"\\n\" + MNBreport)\n",
        "  f.write(\"\\n LR average Accuracy \" + str(LRaccuracy) + \"\\n\" + LRreport)\n",
        "  f.write(\"\\n SVM average Accuracy \" + str(SVMaccuracy) + \"\\n\" + SVMreport)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}